# -*- coding: utf-8 -*-
"""Youtube Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iRxQPJX4Micda5BknIDpqFaNLHLu9dwZ
"""

import pandas as pd
import seaborn as sns

from googleapiclient.discovery import build

api_key = 'AIzaSyAEci1EF6rL2HqeszV1YQs7kz_5EdXoZyw'

youtube = build('youtube', 'v3', developerKey=api_key)

def niche_extraction(query, label, youtube, max_channels=10, max_videos=50):
    # Step 1: Search for Channels by Query
    search_response = youtube.search().list(
        part='snippet',
        q=query,
        type='channel',
        regionCode="US",              # restrict by country
        relevanceLanguage="en",
        maxResults=max_channels
    ).execute()

    channel_ids = [item['id']['channelId'] for item in search_response['items']]

    # Step 2: Get Channel Statistics
    channel_stats_response = youtube.channels().list(
        part='snippet,statistics',
        id=channel_ids
    ).execute()

    channel_data = []
    for item in channel_stats_response['items']:
        channel_data.append({
            'channel_name': item['snippet']['title'],
            'sub_count': int(item['statistics']['subscriberCount'])
        })

    channel_df = pd.DataFrame(channel_data)

    # Step 3: Get Upload Playlist IDs
    content_details_response = youtube.channels().list(
        part='contentDetails',
        id=channel_ids
    ).execute()

    upload_playlist_ids = [
        item['contentDetails']['relatedPlaylists']['uploads']
        for item in content_details_response['items']
    ]

    # Step 4: Fetch Videos from Each Upload Playlist
    all_video_data = []

    for playlist_id in upload_playlist_ids:
        video_response = youtube.playlistItems().list(
            part='snippet,contentDetails',
            playlistId=playlist_id,
            maxResults=max_videos
        ).execute()

        for item in video_response['items']:
            video = {
                'video_name': item['snippet']['title'],
                'video_publish_time': item['snippet']['publishedAt'],
                'video_id': item['contentDetails']['videoId'],
                'channel_name': item['snippet']['channelTitle'],
            }
            all_video_data.append(video)

    video_df = pd.DataFrame(all_video_data)

    # Step 5: Get Video Statistics in Batches of 50
    video_ids = [video['video_id'] for video in all_video_data]
    detailed_video_data = []

    for i in range(0, len(video_ids), 50):
        batch_ids = video_ids[i:i+50]
        id_string = ",".join(batch_ids)

        video_stats_response = youtube.videos().list(
            part="statistics,snippet,contentDetails",
            id=id_string
        ).execute()

        for item in video_stats_response['items']:
            stats = {
                'video_id': item['id'],
                'video_name': item['snippet']['title'],
                'views': item['statistics'].get('viewCount', 0),
                'likes': item['statistics'].get('likeCount', 0),
                'comments': item['statistics'].get('commentCount', 0),
                'duration': item['contentDetails']['duration'],
                'published_date': item['snippet']['publishedAt']
            }
            detailed_video_data.append(stats)

    stats_df = pd.DataFrame(detailed_video_data)

    # Step 6: Merge All DataFrames
    final_df = pd.merge(video_df, stats_df, on='video_id')
    df = pd.merge(final_df, channel_df, on='channel_name', how='left')

    return df

cooking = niche_extraction('Cooking', 'Cooking', youtube)
food_rev = niche_extraction('Food Review', 'Food Review', youtube)
tech_rev = niche_extraction('Tech Reviews', 'Tech Reviews', youtube)
pfinance = niche_extraction('Personal Finance', 'Personal Finance', youtube)
fitness = niche_extraction('Fitness', 'Fitness', youtube)

def cleaning(dataset):
      dataset['video_publish_time'] = pd.to_datetime(dataset['video_publish_time'])
      dataset['published_date'] = dataset['video_publish_time'].dt.date
      dataset['date'] = dataset['video_publish_time'].dt.date
      dataset['hour'] = dataset['video_publish_time'].dt.hour
      def time_slot(h):
        if 5 <= h < 12:
          return 'Morning'
        elif 12 <= h < 17:
          return 'Afternoon'
        elif 17 <= h < 21:
          return 'Evening'
        else:
          return 'Night'
      dataset['time_slot'] = dataset['hour'].apply(time_slot)

      dataset_final = dataset.drop(columns=['published_date','video_publish_time','time', 'video_name_y'])
      return dataset_final

def restore_publish_date(dataset, youtube):
    # Batch process video_ids to avoid hitting API quota
    video_ids = dataset['video_id'].tolist()
    published_dates = {}

    for i in range(0, len(video_ids), 50):
        batch_ids = video_ids[i:i+50]
        id_string = ",".join(batch_ids)

        response = youtube.videos().list(
            part="snippet",
            id=id_string
        ).execute()

        for item in response.get('items', []):
            vid = item['id']
            published_at = item['snippet']['publishedAt']
            published_dates[vid] = pd.to_datetime(published_at).date()

    # Map the dates back into the dataset
    dataset['published_date'] = dataset['video_id'].map(published_dates)
    dataset['date'] = dataset['published_date']

    return dataset


cooking = restore_publish_date(cooking, youtube)
tech_rev = restore_publish_date(tech_rev, youtube)
pfinance = restore_publish_date(pfinance, youtube)
food_rev = restore_publish_date(food_rev, youtube)
fitness = restore_publish_date(fitness, youtube)

cooking_final = cooking.drop(columns=['published_date'])
tech_rev_final = tech_rev.drop(columns=['published_date'])
pfinance_final = pfinance.drop(columns=['published_date'])
food_rev_final = food_rev.drop(columns=['published_date'])
fitness_final = fitness.drop(columns=['published_date'])

cooking = cooking_final.to_csv('cooking.csv', index=False)
tech_rev = tech_rev_final.to_csv('tech_rev.csv', index=False)
pfinance = pfinance_final.to_csv('pfinance.csv', index=False)
food_rev = food_rev_final.to_csv('food_rev.csv', index=False)
fitness = fitness_final.to_csv('fitness.csv', index=False)

cook = pd.read_csv('/content/cook1.csv')
tech = pd.read_csv('/content/tech1.csv')
finance = pd.read_csv('/content/finance1.csv')
food = pd.read_csv('/content/food1.csv')
fit = pd.read_csv('/content/fit1.csv')

"""# **Which cooking videos have the highest views and when were they published?**

"I want to identify what content is performing the best and if timing affects performance."
"""

cooking_videos_with_most_views = cook.sort_values(by='views', ascending=False)[
    ['channel_name','video_name_x','video_id', 'views', 'date', 'hour', 'sub_count']
]



#Every Channel's Highest Viewed Video

idx = cook.groupby('channel_name')['views'].idxmax()
top_videos_per_channel = cook.loc[idx, ['channel_name', 'video_name_x', 'views','sub_count']].sort_values(by='views', ascending=False)
top_videos_per_channel

"""# **Which channels have the most subscribers, and do those channels also have the most views per video?**

**"Does having more subscribers mean better video performance?"**
"""

def vid_performance(dataset):
    channel_stats = dataset.groupby('channel_name').agg({
        'views':'sum',
        'video_id':'count'
    }).rename(columns={'video_id':'total_videos', 'views':'total_views'})

    channel_stats['average_views_per_video'] = channel_stats['total_views'] / channel_stats['total_videos']

    dataset = dataset.merge(channel_stats, on='channel_name')

    dataset['Views_per_sub'] = dataset['average_views_per_video']/dataset['sub_count']
    dataset['Engagement Rate'] = (dataset['likes'] + dataset['comments']) / dataset.groupby('channel_name')['views'].transform('sum')
    dataset['Avg Engagement Rate'] = dataset.groupby('channel_name')['Engagement Rate'].transform('mean')

    return dataset

tech1 = vid_performance(tech)
finance1 = vid_performance(finance)
food1 = vid_performance(food)
fit1 = vid_performance(fit)
cook1 = vid_performance(cook)

cook1['Avg Engagement Rate'] = cook1.groupby('channel_name')['Engagement Rate'].transform('mean')
cook1.head()

fit1 = fit
cook1=cook
tech1=tech
food1=food
finance1=finance

def performance_comparison(dataset):
       dataset_sorting = dataset.groupby(['channel_name','sub_count', 'average_views_per_video','Views_per_sub'])['Avg Engagement Rate'].max().sort_values(ascending=False)
       corr_bw_sub_view = dataset[['sub_count', 'average_views_per_video']].corr()

       return dataset_sorting, corr_bw_sub_view



fit_sorting, fit_corr = performance_comparison(fit1)
cook_sorting, cook_corr = performance_comparison(cook1)
tech_sorting, tech_corr = performance_comparison(tech1)
food_sorting, food_corr = performance_comparison(food1)
finance_sorting, finance_corr = performance_comparison(finance1)

"""# **Can you show me the top 5 videos with the highest like-to-view ratio?**

**"Iâ€™m looking for the most engaging videos, not just the most viewed."**
"""

def like_view(dataset):
    l_v = dataset.groupby(['channel_name', 'video_name_x']).agg({
        'likes': 'sum',
        'views': 'sum'
    }).rename(columns={'likes': 'total_likes_per_channel', 'views': 'total_views_per_channel'})

    l_v = l_v[l_v['total_views_per_channel'] > 0]
    l_v['like_to_view_ratio_percent'] = (l_v['total_likes_per_channel'] / l_v['total_views_per_channel']) * 100


    top_channels = l_v['like_to_view_ratio_percent'].nlargest(5)
    return top_channels

engage_fit = like_view(fit1)
engage_cook = like_view(cook1)
engage_tech = like_view(tech1)
engage_food = like_view(food1)
engage_finance = like_view(finance1)

"""# **What is the average number of views for each channel?**

**"I want to know how each channel is performing on average."**
"""

avg_views_for_cook = cook1.groupby('channel_name')['average_views_per_video'].mean()
avg_views_for_tech = tech1.groupby('channel_name')['average_views_per_video'].mean()
avg_views_for_finance = finance1.groupby('channel_name')['average_views_per_video'].mean()
avg_views_for_food = food1.groupby('channel_name')['average_views_per_video'].mean
avg_views_for_fit = fit1.groupby('channel_name')['average_views_per_video'].mean()

"""# **Whatâ€™s the distribution of comments per video? Are there any videos with unusually high comment counts?**

**"I'd like to spot videos that may have gone viral or sparked conversation."**
"""

comment_for_cook = cook1[['channel_name','video_name_x','views','comments']].sort_values(by='comments', ascending=False).head(10)
comment_for_fit = fit1[['channel_name','video_name_x','views','comments']].sort_values(by='comments', ascending=False).head(10)
comment_for_finance = finance1[['channel_name','video_name_x','views','comments']].sort_values(by='comments', ascending=False).head(10)
comment_for_tech = tech1[['channel_name','video_name_x','views','comments']].sort_values(by='comments', ascending=False).head(10)
comment_for_food = food1[['channel_name','video_name_x','views','comments']].sort_values(by='comments', ascending=False).head(10)

"""# **Which channels have the highest average engagement rate (likes/views)?**

**"We need to find creators who drive high interaction."**
"""

engage_rate_cook = cook1.groupby('channel_name')['Avg Engagement Rate'].max().sort_values(ascending=False)
engage_rate_tech = tech1.groupby('channel_name')['Avg Engagement Rate'].max().sort_values(ascending=False)
engage_rate_finance = finance1.groupby('channel_name')['Avg Engagement Rate'].max().sort_values(ascending=False)
engage_rate_food = food1.groupby('channel_name')['Avg Engagement Rate'].max().sort_values(ascending=False)
engage_rate_fit = fit1.groupby('channel_name')['Avg Engagement Rate'].max().sort_values(ascending=False)

!pip install isodate
import isodate

"""# **Do longer videos get more views, or is there a sweet spot for duration?**

**"Weâ€™re trying to optimize video length â€” whatâ€™s the data say?"**
"""

import isodate

cook1['duration_sec'] = cook1['duration'].apply(lambda x: isodate.parse_duration(x).total_seconds())
tech1['duration_sec'] = tech1['duration'].apply(lambda x: isodate.parse_duration(x).total_seconds())
finance1['duration_sec'] = finance1['duration'].apply(lambda x: isodate.parse_duration(x).total_seconds())
food1['duration_sec'] = food1['duration'].apply(lambda x: isodate.parse_duration(x).total_seconds())
fit1['duration_sec'] = fit1['duration'].apply(lambda x: isodate.parse_duration(x).total_seconds())

cook2 = cook1.drop(columns=['duration'])
tech2 = tech1.drop(columns=['duration'])
finance2 = finance1.drop(columns=['duration'])
food2 = food1.drop(columns=['duration'])
fit2 = fit1.drop(columns=['duration'])
def video_classify(row):
    if row['duration_sec'] < 60:
        return 'Short'
    elif 60 <= row['duration_sec'] < 300:
        return 'Medium'
    else:
        return 'Long'

cook2['video_length_class'] = cook2.apply(video_classify, axis=1)
tech2['video_length_class'] = tech2.apply(video_classify, axis=1)
finance2['video_length_class'] = finance2.apply(video_classify, axis=1)
food2['video_length_class'] = food2.apply(video_classify, axis=1)
fit2['video_length_class'] = fit2.apply(video_classify, axis=1)

fit2.to_csv('fit2.csv', index=False)
cook2.to_csv('cook2.csv', index=False)
tech2.to_csv('tech2.csv', index=False)
finance2.to_csv('finance2.csv', index=False)
food2.to_csv('food2.csv', index=False)


# READ FIT2 CSV

import pandas as pd

fit2 = pd.read_csv('/content/fit2.csv')

cook2 = pd.read_csv('/content/cook2.csv')
tech2 = pd.read_csv('/content/tech2.csv')
finance2 = pd.read_csv('/content/finance2.csv')
food2 = pd.read_csv('/content/food2.csv')

avg_view_for_vid_length_fit = fit2.groupby('video_length_class')['views'].mean()
corr_bw_length_views_fit = fit2[['views', 'duration_sec']].corr()
avg_view_for_vid_length_cook = cook2.groupby('video_length_class')['views'].mean()
corr_bw_length_views_cook = cook2[['views', 'duration_sec']].corr()
avg_view_for_vid_length_food = fit2.groupby('video_length_class')['views'].mean()
corr_bw_length_views_food = food2[['views', 'duration_sec']].corr()
avg_view_for_vid_length_tech = tech2.groupby('video_length_class')['views'].mean()
corr_bw_length_views_tech = tech2[['views', 'duration_sec']].corr()
avg_view_for_vid_length_finance= finance2.groupby('video_length_class')['views'].mean()
corr_bw_length_views_finance = finance2[['views', 'duration_sec']].corr()

"""# **Can you break down view performance by publish month to check seasonal trends?**

**"I want to know if certain times of year perform better for cooking content."**
"""

cook2['date'] = pd.to_datetime(cook2['date'])
cook2['month'] = cook2['date'].dt.month
fit2['date'] = pd.to_datetime(fit2['date'])
fit2['month'] = fit2['date'].dt.month
finance2['date'] = pd.to_datetime(finance2['date'])
finance2['month'] = finance2['date'].dt.month
tech2['date'] = pd.to_datetime(tech2['date'])
tech2['month'] = tech2['date'].dt.month
food2['date'] = pd.to_datetime(food2['date'])
food2['month'] = food2['date'].dt.month

import calendar

def add_month_names(dataset):
    dataset['month_name'] = dataset['month'].apply(lambda x: calendar.month_name[x])
    return dataset



cook2 = add_month_names(cook2)
tech2 = add_month_names(tech2)
finance2 = add_month_names(finance2)
food2 = add_month_names(food2)
fit2 = add_month_names(fit2)

monthly_breakdown = cook2.groupby('month_name')['views'].mean().sort_values(ascending=False)

"""# **How often are videos being uploaded by these channels?**

**"Iâ€™d like to understand upload frequency â€” do the active channels perform better?"**
"""

def calculate_upload_frequency(df):
    df['month_year'] = df['date'].dt.month
    monthly_uploads = df.groupby(['channel_name', 'month']).size().reset_index(name='uploads_in_month')

    upload_freq = monthly_uploads.groupby('channel_name')['uploads_in_month'].mean().reset_index()
    upload_freq = upload_freq.rename(columns={'uploads_in_month': 'avg_uploads_per_month'})

    return upload_freq

upload_freq_cook = calculate_upload_frequency(cook2)
upload_freq_fit = calculate_upload_frequency(fit2)
upload_freq_tech = calculate_upload_frequency(tech2)
upload_freq_food = calculate_upload_frequency(food2)
upload_freq_finance = calculate_upload_frequency(finance2)

"""# **Is there a correlation between publish time (hour) and views?**

**"Should our brand consider uploading at a specific time of day?"**
"""

corr_hr_views_cook = cook2[['views', 'hour']].corr()
corr_hr_views_fit = fit2[['views', 'hour']].corr()
corr_hr_views_tech = tech2[['views', 'hour']].corr()
corr_hr_views_food = food2[['views', 'hour']].corr()
corr_hr_views_finance = finance2[['views', 'hour']].corr()

finance2.head()

finance2.to_csv('finance_final.csv')
tech2.to_csv('tech_final.csv')
food2.to_csv('food_final.csv')
cook2.to_csv('cook_final.csv')
fit2.to_csv('fit_final.csv')

